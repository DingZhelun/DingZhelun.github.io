<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[点云数据集]]></title>
    <url>%2F2019%2F09%2F21%2F%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[老师让我整理一下，点云数据集的一些相关内容 KITTI概述KITTI数据集由德国卡尔斯鲁厄理工学院和丰田美国技术研究院联合创办，是目前国际上最大的自动驾驶场景下的计算机视觉算法评测数据集。该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。KITTI包含市区、乡村和高速公路等场景采集的真实图像数据，每张图像中最多达15辆车和30个行人，还有各种程度的遮挡与截断。整个数据集由389对立体图像和光流图，39.2 km视觉测距序列以及超过200k 3D标注物体的图像组成[1] ，以10Hz的频率采样及同步。总体上看，原始数据集被分类为’Road’, ’City’, ’Residential’, ’Campus’ 和 ’Person’。对于3D物体检测，label细分为car, van, truck, pedestrian, pedestrian(sitting), cyclist, tram以及misc组成。 Dataset详述图-1展示了KITTI数据集的典型样本，分为 ’Road’, ’City’, ’Residential’, ’Campus’ 和’Person’五类。原始数据采集于2011年的5天，共有180GB数据。 数据组织形式论文[2] 中提及的数据组织形式，可能是早期的版本，与目前KITTI数据集官网公布的形式不同，本文稍作介绍。 如图-4所示，一个视频序列的所有传感器数据都存储于data_drive文件夹下，其中date和drive是占位符，表示采集数据的日期和视频编号。时间戳记录在Timestamps.txt文件。 对于从KITTI数据集官网下载的各个分任务的数据集，其文件组织形式较为简单。以Object detection为例，下图是Object Detection Evaluation 2012标准数据集中left color images文件的目录结构，样本分别存储于testing和training数据集。 data_object_image_2|── testing│ └── image_2└── training└── image_2 下图是training数据集的label文件夹目录结构 training/└── label_2 AnnotationsKITTI数据集为摄像机视野内的运动物体提供一个3D边框标注（使用激光雷达的坐标系）。该数据集的标注一共分为8个类别：’Car’, ’Van’, ’Truck’, ’Pedestrian’, ’Person (sit- ting)’, ’Cyclist’, ’Tram’ 和’Misc’ (e.g., Trailers, Segways)。论文[2] 中说明了3D标注信息存储于date_drive_tracklets.xml，每一个物体的标注都由所属类别和3D尺寸（height，weight和length）组成。当前数据集的标注存于每种任务子数据集的label文件夹中，稍有不同。 为了说明KITTI数据集的标注格式，本文以Object detection任务的数据集为例。数据说明在Object development kit的readme.txt文档中。从标注数据的链接 training labels of object data set (5 MB)下载数据，解压文件后进入目录，每张图像对应一个.txt文件。一帧图像与其对应的.txt标注文件如图-3所示。 为了理解标注文件各个字段的含义，需要阅读解释标注文件的readme.txt文件。该文件存储于object development kit (1 MB)文件中，readme详细介绍了子数据集的样本容量，label类别数目，文件组织格式，标注格式，评价方式等内容。下面介绍数据格式的label描述： 注意，’DontCare’ 标签表示该区域没有被标注，比如由于目标物体距离激光雷达太远。为了防止在评估过程中（主要是计算precision），将本来是目标物体但是因为某些原因而没有标注的区域统计为假阳性(false positives)，评估脚本会自动忽略’DontCare’ 区域的预测结果。 Development KitKITTI各个子数据集都提供开发工具 development kit，主要由cpp文件夹，matlab文件夹，mapping文件夹和readme.txt组成。下图以object detection任务的文件夹devkit_object为例，可以看到cpp文件夹主要包含评估模型的源代码evaluate_object.cpp。Mapping文件夹中的文件记录训练集到原始数据集的映射，从而开发者能够同时使用激光雷达点云，gps数据，右边彩色摄像机数据以及灰度摄像机图像等多模态数据。Matlab文件夹中的工具包含读写标签，绘制2D/3D标注框，运行demo等工具。Readme.txt文件非常重要，详述介绍了某个子数据集的数据格式，benchmark介绍，结果评估方法等详细内容。 devkit_object|── cpp│ |── evaluate_object.cpp│ └── mail.h|── mapping│ |── train_mapping.txt│ └── train_rand.txt|── matlab│ |── computeBox3D.m│ |── computeOrientation3D.m│ |── drawBox2D.m│ |── drawBox3D.m│ |── projectToImage.m│ |── readCalibration.m│ |── readLabels.m│ |── run_demo.m│ |── run_readWriteDemo.m│ |── run_statistics.m│ |── visualization.m│ └── writeLabels.m 评价准则Evaluation Metricsstereo与visual odometry任务]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pointnet++理解]]></title>
    <url>%2F2019%2F09%2F17%2Fpointnet-%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/sinat_37011812/article/details/81945050 简介作者在先前的研究中提出Pointnet，此论文是Pointnet的改进版Pointnet++。提出改进的理由是因为Pointnet无法很好地捕捉由度量空间引起的局部结构问题，由此限制了网络对精细场景的识别以及对复杂场景的泛化能力。 Pointnet的基本思想是对输入点云中的每一个点，学习其对应的空间编码，之后再利用所有点的特征得到一个全局的点云特征，这里欠缺了对局部特征的提取及处理，比如说点云空间中临近点一般都具有相近的特征，同属于一个物体空间中的点的概率很大，就好比二维图像中，同一个物体的像素值都相近一样。 再者现实场景中的点云往往是书迷不同的，而Pointnet是基于均匀采样的点云进行训练的，导致了其在实际场景点云中的准确率下降。 Pointnet++就上述提出了改进，解决了两个问题： 如何对点云进行局部划分 如何对点云进行局部特征提取 上述两个问题其实是相互关联的。 实现细节 层级点云特征学习点集的特征提取由三部分组成，分别为Sampling layer、Grouping layer、Pointnet layer。 Sampling layer采样层在输入点云中选择一系列点，由此定义出局部区域的中心。采样算法使用迭代最远点采样方法iterative farthest point sampling（FPS）。FPS：先随机选择一个点，然后再选择离这个点最远的点作为起点，再继续迭代，知道选出需要的个数为止。 相比随机采样，能更完整的通过区域中心点采样到全局点云 Grouping layer目的是要构建局部区域，进而提取特征。思想就是利用临近点，并且论文中使用的是neighborhood ball，而不是KNN，是因为可以保证有一个fixed region scale，主要的指标还是距离distance。 Pointnet layer在如何对点云进行局部特征提取的问题上，利用原有的Pointnet就可以很好的提取点云的特征，由此在Pointnet++中，原先的Pointnet网络就成为了Pointnet++网络中的子网络，层级迭代提取特征。 点云密度不均匀时的鲁棒特征学习这里作者解决空间中点云的密度不均匀对特征学习带来的挑战，提出了两种grouping的方法，即如何提取不同尺度的局部patterns并按照局部点的密度去组合它们。称为密度自适应层 Multi-scale Grouping简单而有效的方式，直接对不同密度的点云特征（通过Pointnet提取后的）进行组合 不同密度的点云时通过对输入点云进行不同概率的dropout得到的 问题是计算量比较大 Multi-resolution Grouping分两部分，一部分直接用Pointnet从raw points上提取特征，另一部分是对subregion使用set abstraction得到的特征的集合。第一部分想当于是一个比较全局的部分，第二部分相当于是一个比较局部的部分，这里用两部分可以很好的控制全局区域密度。当局部区域密度比较小时，说明全局特征没有全局特征可靠，因此可以增加全局特征的权重。反之也是如此。这就相当于权重可以在密度的变化之中可以被学习到。 Segmentation在网络中输入不断被降采样而在segmentation中label都是针对原始点的，相当于需要做一个upsampling的动作。作者使用插值的方法再和之前的set abstraction中的feature做一个concatenate，inverse distance weighted average based on k nearest neighbors 结论Pointnet++的结构在3D point clouds上取得了state of art的水平，解决了如何处理采样不均匀的问题，也考虑了空间中点与点之间的距离度量，通过层级结构利用局部区域信息学习特征，网络结构更有效更鲁棒。]]></content>
      <tags>
        <tag>Pointnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令]]></title>
    <url>%2F2019%2F09%2F14%2Flinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一些用到的Linux命令 清华镜像pip install -i https://pypi.tuna.tsinghua.edu.cn/simple top命令TOP是一个动态显示过程,即可以通过用户按键来不断刷新当前状态。如果在前台执行该命令,它将独占前台,直到用户终止该程序为止。比较准确的说,top命令提供了实时的对系统处理器的状态监视。它将显示系统中CPU最“敏感”的任务列表。该命令可以按CPU使用。内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。 nvidia-smi 命令nvidia-smi 查看gpu的使用情况：]]></content>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PointNet理解]]></title>
    <url>%2F2019%2F08%2F16%2FPointNet%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[初学pointnet，一些重点笔记 一、三维深度学习简介 多视角（multi-view）：通过多视角二维图片组合三维物体，此方法将传统CNN应用于多张二维视角的图片，特征被view pooling procedure聚合起来形成三维物体 体素（volumetric）：通过将物体表现为空间中的体素进行类似于二维的三维卷积（例如，卷积核大小为5*5*5），是规律化的并且易于类比二维的，但同时因为多了一个维度出来，时间和空间复杂度都非常高，目前已经不是主流的方法了 点云（point clouds）：直接将三维点云抛入网络进行训练，数据量小。主要任务有分类、分割以及大场景下的语义分割 非欧式（manifold graph）：在流形或图的结构上进行卷积，三维点云可以表现为mesh结构，可以通过点对之间临接关系表现为图的结构。流形表达比较抽象，用到拉普拉斯特征什么的 二、点云存在的问题1.无序性：点云本质上是一长串店（n*3矩阵，其中n是点数）。在几何上，点的顺序不影响它在空间中对整体形状的表示，例如，相同的点云可以由两个完全不同的矩阵表示。如下图左边所示： 我们希望得到的效果图如下图右边：N代表点云个数，D代表每个点的特征维度，不论点云顺序怎样，希望得到相同的特征提取结果 我们知道，网络的一般结构是：提取特征-特征映射-特征图压缩（降维）-全连接 下图中x代表点云中的某个点，h代表特征提取层，g叫做对称方法，r代表更高维特征提取，最后接一个sofmax分类。g可以是maxpooling或sumpooling，也就是说，最后的D维特征对每一维都选取N个点中对应的最大特征值或特征值总和，这样就可以通过g来解决无序性问题。pointnet采用了max-pooling策略 2.旋转性：相同的点云在空间中经过了一定的刚性变化（旋转或平移），坐标发生变化，如下图所示： 我们希望不论点云在怎样的坐标系下呈现，网络都能正确的识别出。这个问题可以通过STN(spacial transform networks)来解决。二维的变换方法可以参考这里，三维不太一样的是点云是个不规则的结构（无序，无网格），不需要重采样过程。pointnet通过学习一个矩阵来达到对目标最有效的变换。 三、pointnet网络结构详解先来看看网络的两个亮点： 空间变换网络（STN）解决旋转问题：三维的STN可以通过学习点云本身的位姿信息学习到一个最有利于网络进行分类或分割的D*D旋转矩阵（D代表特征维度，pointnet中D采用3和64）。至于其中的原理，我的理解是，通过控制最后的loss来对变换矩阵进行调整，pointnet并不关心最后真正做了什么变换，只要有利于最后的结果都可以。pointnet采用了两次STN，第一次input transform是对空间中的点云进行调整，直观上理解是旋转出一个更有利于分类或分割的角度，比如，把物体转到正面；第二次feature transform是对提取出的64维特征进行对齐，即在特征层面对点云进行变换 maxpooling解决无序性问题：网络对每个点进行了一定程度的特征提取之后，maxpooling可以对点云的整体提取出global feature 再来看网络结构： 其中mlp是通过共享权重的卷积实现，第一层卷积核大小是13（因为每个点的维度是xyz），之后的每一层卷积核大小都是1\1。即特征提取层知识把每个点连接起来而已。经过两个STN和两个mlp之后，对每个点提取1024维特征，经过maxpooling变成1*1024的全局特征。再经过一个mlp(代码中运用全连接)得到k个score。分类网络最后接的loss是softmax 四、pointnet代码详解重点已经框出 网络模型部分 变换矩阵部分，以第一个STN为例]]></content>
      <tags>
        <tag>PointNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度图、网格、体素、点云]]></title>
    <url>%2F2019%2F08%2F16%2F%E6%B7%B1%E5%BA%A6%E5%9B%BE%E3%80%81%E7%BD%91%E6%A0%BC%E3%80%81%E4%BD%93%E7%B4%A0%E3%80%81%E7%82%B9%E4%BA%91%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/lainey/p/8547056.html 深度图Depth map 深度图是一张2D图片，每个像素都记录了从视点到遮挡物表面（遮挡物就是阴影生成物体）的距离，这些像素对应的顶点对于观察者来说是“可见的”。 Depth map中像素点记录的深度值为lenth1；然后从视点出现，计算物体顶点v到视点的距离，记为lenth2；比较二者大小，来确定“v”是否被遮挡。该术语的同义词有depth buffer，Z-buffer，Z-buffering和Z-depth。这里的“Z”是相对于相机（即视点）视图中心轴而言的，也就是相机的z轴线，而不是场景的绝对坐标中的z轴线。 用途 模拟在一个场景中的密度均匀的半透明介质效果-如雾，烟或大量的水 模拟场景表面的深度域（depth of field (DOF)） 可用于高效的变形体碰撞检测 体素（立体像素）体素或立体像素（voxel），是体积像素（volume pixel）的简称。概念上类似二维空间的最小单位——像素，像素用在二维电脑图像的视频数据上。体积像素一如其名，是数字数据于三维空间分区上的最小单位，应用于三维成像、科学数据与医学视频等领域。有些真正的三维显示器运用体素来描述他们的分辨率，举例说：可以显示512×512×512体素的显示器。 如同像素，体素本身并不含有空间中位置的数据（即他们的坐标），然而却可以从他们相对于其他体素的位置推敲，意即他们在构成单一张体积视频的数据结构中的位置。 网格多边形网络（Polygon mesh）是三维计算机图形学中表示多面体形状的顶点与多边形的集合,它也叫作非结构网格。 这些网格通常由三角形、四边形或者其他的简单凸多边形组成，这样可以简化渲染过程。但是，网格也可以包括带有空洞的普通多边形组成的物体。 非结构网格内部表示的例子有： 一组顶点的简单列表，它们带有表示那些顶点组成多边形的信息列表；另外可能带有表示空洞的附加信息 顶点列表 + 边界列表（一对索引信息）+ 连接边界的多边形列表 翼边数据结构 根据应用程序的不同所选择的数据结构也有所不同：三角形的处理要比普通多边形的处理更加简单，尤其是在计算几何中更是这样。对于优化的算法，可能需要快速访问边线或者相邻表面这样的拓扑信息，这样就需要如翼边表示这样更加复杂的结构。 点云（维基百科）点云（point cloud）是指通过3D扫描器所取得至资料形式。 扫描资料以点的型式记录，每一个点包含有三维座标，有些可能含有色彩资讯（R,G,B）或物体反射面强度。 点云数据除了具有几何位置以外，还有强度（Intensity）信息，强度信息的获取是激光扫描仪接受装置采集到的回波强度，此强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关。点云也是逆向工程中通过仪器测量外表的点数据集合。 在电脑动画领域，皮克斯的玩具总动员3使用了点云技术。 点云应用深度学习面临的挑战： 非结构化数据，不变性排列，点云数据量上的变化(不同传感器上点云的数量变化很大) 点云数据方面的挑战： 缺少数据：扫描的模型通常被遮挡，部分数据丢失 噪音：所有传感器都是嘈杂的。有几种类型的噪声，包括点云扰动和异常值。这意味着一个点有一定的概率位于它被采样的地方(扰动)附近的某一半径范围内，或者它可能出现在空间的任意位置(异常值) 旋转：一辆车向左转，同一辆车向右转，会有不同的点云代表同一辆车 在点云上直接用深度学习的方法是将数据转换成体积表示，比如体素网格，然后就可以用3D滤波器来训练CNN，但是体积数据会变得非常大，3D CNN处理会非常慢，所以需要妥协到较低的分辨率，就会带来量化误差的代价。 针对无序点云数据的深度学习方法研究进展缓慢，主要有三个方面： 点云具有无序性 点云具有稀疏性 —— 在KITTI数据集中，如果把原始的激光雷达点云投影到对应的彩色图像上，大概只有3%的像素才有对应的雷达点。这种极强的稀疏性让基于点云的高层语义感知变得尤其困难。 点云信息量有限 —— 点云的数据结构就是一些三维空间的点坐标构成的点集，本质是对三维世界几何形状的低分辨率重采样，因此只能提供片面的几何信息 举例说明 iphoneX 3D结构光双摄 用的是PrimeSense的结构光深度重建方案(Depth),和普通的彩色RGB不同，深度摄像头输出的时RGBD图像，多了一个深度通道，深度图像看起来是这样的：由深度图可以得到点云，进而得到网格（mesh）效果如下： 3D 重建后的人脸，比 2D人脸多了很多信息，识别显然会更准确 由于结构光的特性，每一帧数据都能重建出完整的 3D 模型，速度也非常快，才能适应像手机解锁这样的应用 结构光方案因为自带光源，所以天黑的时候也能用 最大池化 max pooling https://blog.csdn.net/u012193416/article/details/79432668 池化操作时在卷积神经网络中经常采用过的一个基本操作，一般在卷积层后面都会接一个池化操作，但是近些年比较主流的ImageNet上的分类算法模型都是使用的max-pooling，很少使用average-pooling，这对我们平时设计模型时确实有比较重要的参考作用，但是原因在哪里呢？ 通常来讲，max-pooling的效果更好，虽然max-pooling和average-pooling都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性，根据相关理论，特征提取的误差主要来自两个方面： 邻域大小受限造成的估计值方差增大 卷积层参数误差造成估计均值的偏移 一般来说，average-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。average-pooling更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说DenseNet中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。 但是average-pooling在全局平均池化操作中应用也比较广，在ResNet和Inception结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一位向量。 max-pooling和average-pooling的使用性能对于我们设计卷积网络还是很有用的，虽然池化操作对于整体精度提升效果也不大，但是在减参，控制过拟合以及提高模型性能，节约计算力上的作用还是很明显的，所以池化操作时卷积设计上不可缺少的一个操作。]]></content>
      <tags>
        <tag>点云</tag>
      </tags>
  </entry>
</search>
