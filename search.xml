<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Frustum PointNets]]></title>
    <url>%2F2019%2F10%2F12%2FFrustum-PointNets%2F</url>
    <content type="text"><![CDATA[Frustum PointNets for 3D Object Detection from RGB-D Data摘要在这项工作中，我们研究了室内和室外场景中基于RGB-D数据的3D对象检测。 尽管以前的方法主要关注图像或3D体素，这些图像通常会掩盖自然的3D模式和3D数据的不变性，但我们通过弹出RGB-D扫描直接对原始点云进行操作。但是，这种方法的主要挑战是如何有效地在大型场景的点云中定位对象（region proposal）。我们的方法不单单依靠3D提议，而是利用成熟的2D对象检测器和高级3D深度学习来进行对象定位，从而即使是很小的对象也可以实现高效率和高召回率。得益于原始点云数据上直接学习，我们的方法在强遮挡和点非常稀疏的情况下，也能准确的估计3D物体边界。在KITTI和SUN RGB-D 3D检测基准上进行了评估，我们的方法在具有实时能力的同时，以明显的优势超越了现有技术。 简介近来，在2D图像理解任务（例如对象检测[13]和实例分割[14]）上已经取得了很大的进步。然而，除了获取2D边界或像素蒙版外，在像自动驾驶和增强现实等一些应用中，急需3D理解。随着部署在移动设备和自动汽车上的3D感知器的普及，越来越多的3D数据被捕获和处理。在这项工作中，我们研究了最重要的3D感知任务之一——3D对象检测，该任务根据3D传感器数据，对物体进行分类和估算物体对象的定向3D边界框。 尽管3D传感器数据通常以点云的形式出现，但是如何表示点云数据和对3D物体检测使用什么样的深度网络结构仍然是一个开放的问题。大多数现有的作品通过投影[36,26]将3D点云转换为图像，或通过量化[40.23.26]转换为体积网络，然后应用于体积网络。 图1.3D物体检测管道。给出RGB-D数据，我们首先在RGB中使用CNN产生2D区域建议。然后将每一个2D区域挤出到3D视锥中，在该视锥中，我们从深度数据中获得点云。最终，我们的frustum PointNet根据视锥中的点预测了（定向和无模态）对象的3D边界框。 然而，这种数据表示形式的转换可能使自然3D模型和数据的不变性模糊不清。最近，许多论文提出了直接处理点云而不转换数据到其他的格式。例如，[25,27]提出了一种新型的深度网络架构，叫Pointnets，这种模型在3D理解任务（例如对象分类和语义分割）表现出卓越的性能和效率。 虽然Pointnets能够对整个点云进行分类，或为点云中的每个点预测语义类别，但是尚不清楚如何将该架构用于实例级3D物体检测。为了实现这个目标，我们必须解决一个关键问题：如何有效的提出在3D空间中3D物体的有效位置。模仿图像检测的实践，可以通过滑动窗口[8]或通过像[33]的3D区域建议网络枚举候选3D框。然而， 但是，3D搜索的计算复杂度通常相对于分辨率呈三次方增长，并且对于大型场景或实时应用（例如自动驾驶）而言过于昂贵。 相反，在这项工作中，我们遵循降维原理减少了搜索空间：我们采用了成熟的2D对象检测器（图1）。首先，通过从图像检测器中拉伸2D边界框来提取对象的3D边界视锥。然后，在每个3D视锥所修剪的3D空间中，我们使用Point-Net的两种变体连续执行3D对象实例分割和无模3D边界框回归。分割网络可预测感兴趣对象的3D遮罩（即实例分割）； 然后回归网络估算无模3D边界框（即使只有一部分可见，也可以覆盖整个对象）。 与之前的将RGB-D数据视为2D映射的CNNs相比，当我们将深度图提升到3D点云并用3D工具处理它们时，我们的方法更加以3D为中心。这种以3D为中心的视图可以更加有效的探索3D数据。首先，在我们的pipeline中，对3D坐标成功地应用了一些变换，这些变换把点云校准到一序列更受约束的规范框架中。这些对齐方式排出了数据中姿势的差异，并因此使3D几何图形更加明显，从而使3D学习者的工作更加轻松。其次，在3D空间中学习能够更好的挖掘3D空间中的几何和拓扑结构。原则上，所有的物体都存在于3D空间中；因此，我们相信，直接在3D空间中运行学习者可以更自然的对一些几何结构（例如，重叠、平面和对称）进行参数话和捕获。最近的实验证据已经证明了这种以3D为中心的网络设计理念的实用性。 我们的方法在KITTI 3D对象检测[1]和鸟瞰视图[2]基准测试中均处于领先地位。与以前的现有技术相比[6]，我们的方法比3D车载AP上具有更高的效率（以5 fps的速度运行），要好8.04％。 我们的方法也非常适合室内RGB-D数据，在此条件下，我们的3D mAP比SUN-RGBD上的[16]和[30]分别提高了8.9％和6.4％，而运行速度却快了1至3个数量级。 ​ 我们的工作的主要贡献如下： * 我们为基于RGB-D数据的3D对象检测提出了一种新颖的框架，称为Frustum PointNets。 * 我们展示了如何在我们的框架下训练3D对象检测器，并在标准3D对象检测基准上达到最先进的性能。 * 我们提供广泛的定量评估，以验证我们的设计选择以及丰富的定性结果，以了解我们方法的优势和局限性 相关工作3D Object Detection from RGB-D Data（从RGB-D数据进行3D对象检测） 研究人员通过采用各种方式表示RGB-D数据来解决3D检测问题。 ​ Front view image based methods（基于正视图图像的方法）：[4,24,41]拍摄单眼RGB图像并塑造先验或遮挡模式以推断3D边界框。[18，7]将深度数据表示为2D映射，并应用CNN来定位2D图像中的对象。相比之下，我们将深度表示为点云，并使用可以更有效地探索3D几何形状的高级3D深度网络（PointNets）。 ​ Bird’s eye view based methods（基于鸟瞰的方法）：MV3D [6] 将LiDAR点云投影到鸟瞰图上，并为3D边界框训练一个区域提议。然而，这种方法在检测小型物体（例如，行人和骑自行车的人）方面滞后，并且不能轻易的适应垂直方向上具有多个物体的场景。 ​ 3D based methods（基于3D的方法）：[38,34]利用SVM从点云中提取的手工设计的几何特征上训练3D对象分类器，然后使用滑动窗口搜索对对象进行定位。[8]通过在体素化3D网格上用3D CNN替换SVM扩展了[38]。[30]为点云中的3D对象检测设计了新的几何特征。[35，17]将整个场景的点云转换为体积网格，并使用3D体积CNN进行对象建议和分类。由于3D卷积的昂贵成本和较大的3D搜索空间，这些方法的计算成本通常很高。最近，[16]提出了一种2D驱动的3D对象检测方法，该方法与我们的想法类似。但是，他们使用手工制作的特征（基于点坐标的直方图）和简单的完全连接的网络来回归3D框的位置和姿势，这在速度和性能上都不理想。相反，我们提出了使用深度3D特征学习（PointNets）的更灵活和有效的解决方案。 Deep Learning on Point Clouds（点云上的深度学习） 大多数现有的作品在特征学习之前将点云转换为图像或体积形式。[40，23，26]将点云体素化为体积网格，并将图像CNN泛化为3D CNN。 [19，31，39，8]设计了更有效的3D CNN或利用点云稀疏性的神经网络架构。但是，这些基于CNN的方法仍然需要对具有特定体素分辨率的点云进行量化。最近，一些著作[25，27]提出了一种新型的网络体系结构（PointNets），该体系结构直接消耗原始点云而无需将其转换为其他格式。虽然PointNet已应用于单个对象分类和语义分段，但我们的工作探索了如何扩展其结构，以实现3D对象检测。 问题定义给定RGB-D数据作为输入，我们的目标是对3D空间中的对象进行分类和定位。从LiDAR或室内深度传感器获得的深度数据表示为RGB相机坐标中的点云。投影矩阵也是已知的，因此我们可以从2D图像区域获得3D视锥。每个对象都由一个类别（k个预定义类别中的一个）和一个无模态3D边界框表示。即使部分对象被遮挡或截断，无模式框也会标记整个对象。通过3D框的大小$h,w,l$，中心${c_x},{c_y},{c_z}$和相对于每个类别的预定义规范姿势的方向$\theta ,\varphi ,\psi$来参数化。在我们的实现中，我们仅考虑围绕上轴的方位角$\theta$进行定向。 图2.用于3D对象检测的Frustum PointNets。我们首先利用2D CNN对象检测器来提出2D区域并对其内容进行分类。然后将2D区域提升为3D，从而成为锥体提议。给定一个锥体中的点云（具有$n$个点的$n×c$和$XYZ$、每个点的强度等的$c$通道，），通过对每个点进行二分类来分割对象实例。基于分段的对象点云（$m×c$），轻量级回归PointNet（T-Net）尝试通过平移对齐点，以使它们的质心靠近模态框中心。最后，框估计网络估计对象的无模3D边界框。图4和图5给出了有关所涉及的坐标系以及网络输入，输出的更多图示。 3D Detection with Frustum PointNets如图2所示，我们的3D对象检测系统包括三个模块：视锥提案，3D实例分割和3D非模态边界框估计。我们将在以下小节中介绍每个模块。我们将专注于每个模块的管道和功能，并向读者推荐有关所涉及的深层网络的特定结构的补充。 Frustum Proposal 视锥提案大多数3D传感器（尤其是实时深度传感器）产生的数据分辨率仍低于商用相机的RGB图像。 因此，我们利用成熟的2D对象检测器在RGB图像中提出2D对象区域以及对对象进行分类。 使用已知的相机投影矩阵，可以将2D边界框提升到一个视锥（视深度传感器范围指定了近平面和远平面），以定义对象的3D搜索空间。 然后，我们收集视锥中的所有点以形成视锥点云。如图4（a）所示，截头可能会朝向许多不同的方向，这会导致点云的位置发生很大的变化。如图4（a）所示，视锥可能会朝向许多不同的方向，这会导致点云的位置发生很大的变化。因此，我们通过将视锥朝向中心视图旋转以使视锥的中心轴正交于像平面，从而对视锥进行标准化。这种归一化有助于改善算法的旋转不变性。我们称此为从RGB-D数据视锥提案生成中提取视锥点云的整个过程。 尽管我们的3D检测框架无法确定2D区域提议的确切方法，但我们采用基于FPN [20]的模型。尽管我们的3D检测框架无法确定2D区域提议的确切方法，但我们采用基于FPN [20]的模型。我们在ImageNet分类和COCO对象检测数据集上预先训练模型权重，然后在KITTI 2D对象检测数据集上对其进行进一步微调，以分类和预测无模式2D框。补充中提供了2D检测器训练的更多详细信息。 图3.在视锥点云中进行3D检测的挑战。左：带有人像区域建议的RGB图像。右：从2D盒中挤出的视锥中的LiDAR点的鸟瞰图，在这里，前景遮挡物（自行车）和背景杂波（建筑物）的点分布广泛。 图4.点云的坐标系统。显示了人工点（黑点）以说明（a）默认摄像机坐标；（b）将视锥旋转到中心视图后的视锥坐标（第4.1节）；（c）以原点的质心为中心的蒙版坐标（第4.2节）； （d）T-Net预测的对象坐标（第4.3节）。 图5. PointNet的基本结构和IO。说明了PointNet ++ [27]（v2）模型的结构，该模型具有集合抽象层和特征传播层（用于分段）。所涉及的协调系统如图4所示。 3D Instance Segmentation 3D实例分割给定2D图像区域（及其对应的3D轮廓），可以使用几种方法来获取对象的3D位置：一种简单的解决方案是直接回归3D对象的位置（例如，通过3D边界框） 使用2D CNN的深度图。但是，此问题并不容易，因为在自然场景中（通常如图3所示）遮挡对象和背景杂乱的现象很常见，这可能会严重分散3D定位任务的注意力。因为对象在物理空间中是自然分离的，远处的物体在图像中可能会紧挨着，所以3D点云中的分割要比图像中的分割更加自然和容易。观察到这一事实后，我们建议在3D点云中分割实例，而不是在2D图像或深度图中。类似于Mask-RCNN [14]，通过对图像区域中的像素进行二分类来实现实例分割，我们在视锥上的点云上使用基于PointNet的网络实现了3D实例分割。 3D Instance Segmentation PointNet（3D实例分割PointNet） 网络在视锥中获取点云，并预测每个点的概率得分，该概率得分指示该点属于感兴趣对象的可能性。注意，每个锥体只包含一个感兴趣的对象。在这里，这些“其他”点可以是无关区域（例如地面，植被）的点，也可以是遮挡或位于感兴趣对象后面的其他实例。与2D实例分割中的情况类似，视视锥的位置而定，一个视锥中的对象点可能会变得混乱或闭塞另一视锥中的点。因此，我们的分割PointNet正在学习遮挡并聚合模式，以及识别特定类别对象的几何形状。 在多类别检测的情况下，我们还利用2D检测器的语义进行更好的实例分割。例如，如果我们知道感兴趣的对象是行人，则分割网络可以使用它来查找看起来像人的几何形状。具体而言，在我们的结构中，我们将语义类别编码为one-hot向量（对于预定义的k个类别为k维），并将one-hot向量连接到中间点云特征。补充中描述了特定结构的更多详细信息。 在3D实例分割之后，被分类为感兴趣对象的点被提取出来（图2中的“masking”）。获得这些分割后的对象点后，我们将其坐标进一步归一化，以提高算法的平移不变性， 基本原理与视锥提案的步骤相同。在我们的实现中，我们通过将质点的XYZ值减去，将点云转换为局部坐标。这在图4（c）中示出。请注意，我们有意不对点云进行缩放，因为局部点云的边界范围的大小会受到视点的极大影响，并且点云的实际大小有助于估计框的大小。 在我们的实验中，我们发现坐标转换（例如上面的坐标转换和以前的视锥旋转）对于3D检测结果至关重要，如表8所示。 Amodal 3D Box Estimation 非模态的3D框估计给出分段的对象点（在3D蒙版坐标中），此模块通过使用框回归PointNet和预处理转换（T-Net？）网络来估计对象的面向无模态3D边界框。 Learning-based 3D Alignment by T-Net 通过T-Net进行基于学习的3D对齐 即使我们已经根据对象的质心位置对齐了分割的对象点，我们仍然发现蒙版坐标框的原点（图4（c））可能仍然距离无模态盒中心很远。 轻量级回归PointNet（T-Net）估计完整对象的真实中心，然后转换坐标，使预测的中心成为原点（图4（d））。因此，我们建议使用轻量级回归PointNet（T-Net）估计完整对象的真实中心，然后变换坐标，使预测的中心成为原点（图4（d））。 我们的T-Net的结构和训练类似于[25]中的T-Net，可以将其视为一种特殊类型的空间转换网络（STN）[15]。但是，与没有直接监督变换的原始STN不同，我们明确监督转换网络，以预测从蒙版坐标原点到真实对象中心的中心残差。 Amodal 3D Box Estimation PointNet 非模态3D框估算PointNet 框估计网络（对于整个对象，即使部分看不见）在给定3D对象坐标中的对象点云的情况下，为对象预测非模态包围框（图4（d））。网络架构类似于对象分类[25，27]，但是输出不再是对象类别分数，而是3D边界框的参数。 如第二节所述。如图3所示，我们通过其中心 $({c_x},{c_y},{c_z})$，大小$(h，w，l)$和航向角$\theta $（沿上轴）对3D边界框进行参数化。我们采用“残差”的方法进行边框中心估计。框估计网络预测的中心残差与来自T-Net和掩盖点的质心的先前中心残差相结合，以恢复绝对中心（等式1）。对于边框的大小和航向角，我们遵循先前的工作[29，24]，并使用分类和回归公式的混合体。具体来说，我们预定义了$N S$大小的模板和$N H$个等分角箱。我们的模型将尺寸/标题（尺寸的$NS$分数，标题的$NH$分数）分类为那些预定义的类别，并预测每种类别的剩余数（$3×NS$剩余尺寸的高度，宽度，长度，$NH$航向的剩余角度）。最后，净输出总共$3 + 4×NS + 2×NH$个数。 $${C_{pred}} = {C_{mask}} + {\rm{ }}\Delta {C_{t - net}} + {\rm{ }}\Delta {C_{box - net}}$$ Training with Multi-task Losses我们同时优化了涉及多任务损失的三个网络（3D实例分割PointNet，T-Net和无模态框估计PointNet）（如公式2所示）。$L_{c1-reg}$用于T-Net，$L_{c2-reg}$用于框估计网络的中心回归。 $L_{h-cls}$和$L_{h-reg}$是航向角预测的损失，而$L_{s-cls}$和$L_{s-reg}$是框尺寸的损失。 Softmax用于所有分类任务，smooth-$l_1$（huber）损失用于所有回归情况。 $$L_{multi-task}=L_{seg}+\lambda(L_{c1-reg}+L_{c2-reg}+L_{h-cls}+L_{h-reg}+L_{s-cls}+L_{s-reg}+\gamma L_{corner}$$ (2) Corner Loss for Joint Optimization of Box Parameters 联合优化箱形参数的角损失 尽管我们的3D边界框参数化的紧凑并且完整，但学习并未针对最终3D框的精度进行优化——中心，尺寸和朝向具有单独的损耗项。想象一下这样的情况：可以准确地预测中心和尺寸，但是航向角是不对的——带有基本事实框的3D IoU将由角度误差决定。理想情况下，应该对所有三个项（中心，大小，航向）进行联合优化，以实现最佳3D框估计（在IoU度量标准下）。为了解决这个问题，我们提出了一种新颖的正则化损失，即角损失： $${L_{corner}} = \sum\limits_{i = 1}^{NS} {\sum\limits_{j = 1}^{NH} {{\delta _{ij}}\min \{ \sum\limits_{k = 1}^8 {||P_k^{ij} - P_k^*||,\sum\limits_{i = 1}^8 {||P_k^{ij} - P_k^{**}||} } \} } } $$ （3） 本质上，角损失是预测框和基本事实框的八个角之间的距离之和。由于拐角位置是由中心，大小和航向共同确定的，因此角损失能够规范针对这些参数的多任务训练。 为了计算角损失，我们首先从尺寸的模板和航向角箱构造$N S×N H$“锚“边框。然后将锚边框转换为估计箱中心。我们将锚边框角表示为$P_k^{ij}$，其中$i,j,k$分别为尺寸类别，朝向类别和（预先定义的）角顺序的索引。为了避免翻转朝向估计带来的较大的损失，我们进一步计算了从翻转基本事实框到角的距离（$P_k^{**}$），并使用原始案例和翻转后案例的最小值。$\delta_{ij}$，是基本真实大小/朝向类别的一个，其他为零，是一个二维掩码，用于选择我们关心的距离项。 实验实验被分为三部分。第一部分，我们比较了在KITTI [10]和SUN-RGBD [33]上进行3D对象检测的最新方法（第5.1节）。第二部分，我们提供深入的分析以验证我们的设计选择（第5.2节）。最后，我们展示出定性结果并讨论了我们方法的优势和局限性（第5.3节）。 Comparing with state-of-the-art Methods 与最先进的方法进行比较我们根据3D对象检测的KITTI [11]和SUN-RGBD [33]基准评估我们的3D对象检测器。与最新方法相比，在这两项任务上我们都取得了明显更好的结果。 KITTI 表1显示了我们的3D检测器在KITTI测试集上的性能。我们在很大程度上优于以前的最新方法。MV3D [6]使用多视图特征聚合和复杂的多传感器融合策略时，我们基于PointNet [25]（v1）和PointNet ++ [27]（v2）主干的方法在设计上更为简洁。 尽管超出了这项工作的范围，但我们希望传感器融合（尤其是3D检测的图像特征聚合）可以进一步改善我们的结果。 我们还在表2中显示了该方法在3D对象定位（鸟瞰）上的性能。在3D定位任务中，将边界框投影到鸟瞰平面，并在定向的2D框中评估IoU。同样，我们的方法大大优于以前的工作，包括DoBEM [42]和MV3D [6]在投射的LiDAR图像上使用CNN，以及在体素化的点云上使用3D CNN的3D FCN [17]。 表1. KITTI测试集上的3D对象检测3D AP。DoBEM [42]和MV3D [6]（先前最先进的技术）基于具有鸟瞰LiDAR图像的2D CNN。在没有传感器融合或多视图聚合的情况下，我们的方法在所有类别和数据子集上的性能大大优于那些方法。3D边界框IoU阈值对于汽车是70％，对于行人和骑自行车的人是50％。 表2. KITTI测试集上的3D对象定位AP（鸟瞰图）。 3D FCN [17]在体素化点云上使用3D CNN，远非实时。MV3D [6]是先前最先进的技术。我们的方法在所有类别和数据子集上均明显优于那些方法。鸟瞰2D边界框IoU阈值对于汽车是70％，对于行人和骑自行车的人是50％。 表3. KITTI val set上的3D对象检测AP（仅限汽车）。 表4. KITTI val set上的3D对象定位AP（仅限汽车）。 表5. 行人和骑自行车者的KITTI val设置的性能。评估的模型是Ours（v2）。 图6中显示了我们网络的输出，即使在非常困难的情况下，我们也可以观察到准确的3D实例分割和框预测。我们将有关成功和失败案例模式的更多讨论推迟到了5.3节。我们还报告了表3和表4（用于汽车）中的KITTI val set（与[6]相同的划分）的性能，以支持与更多已发表的作品进行比较，并在表5（用于行人和骑自行车的人）中进行参考。 SUN-RGBD 之前的大多数3D检测工作要么专门用于室外LiDAR扫描，在这种情况下，对象在空间中间隔得很好，并且点云稀疏（因此对于鸟眼投影是可行的），或者在室内深度图上是具有密集像素值的常规图像， 图像CNN可以轻松应用。 但是，对于在垂直空间中经常同时存在多个物体的室内房间而言，为鸟瞰设计的方法可能无法使用。另一方面，室内聚焦方法很难从LiDAR扫描中应用于稀疏和大规模的点云。 相反，我们基于视锥的PointNet是用于室外和室内3D对象检测的通用框架。通过应用与KITTI数据集相同的流水线，我们在SUN-RGBD基准测试（表6）上获得了最先进的性能，具有显著更高的mAP以及更快的（10x-1000x）推理速度 。 Architecture Design Analysis 结构设计分析在本节中，我们提供分析和消融实验以验证我们的设计选择。 Experiment setup 实验设置 除非另有说明，否则本节中的所有实验均基于我们的v1模型，该模型基于KITTI数据，使用[6]中的train/val划分。为了消除2D检测器的影响，我们将基本事实2D框用于区域建议，并使用3D框估计精度（IoU阈值0.7）作为评估指标。我们只关注训练最多的汽车类别。 图6.在KITTI val集上的Frustum PointNet结果的可视化（最佳放大显示颜色）。这些结果基于PointNet ++模型[27]，其以5 fps的速度运行，并且对汽车，行人和骑自行车的人实现的测试集3D AP分别为70.39、44.89和56.77。点云上的3D实例蒙版以彩色显示。 真阳性检测框为绿色，假阳性框为红色，基本事实框为蓝色，分别表示假阳性和假阴性情况。每个框旁边的数字和字母表示实例ID和语义类别，其中“ v”代表汽车，“ p”代表行人，“ c”代表骑车人。参见5.3节有关结果的更多讨论。 表6. SUN-RGBD val集合上的3D对象检测AP。评估指标是[33]提出的3D IoU阈值为0.25的平均精度。请注意，COG [30]和2D驱动的[16]都使用房间布局上下文来提高性能，而我们和DSS [35]则没有。与以前的最新技术相比，我们的方法在mAP方面提高了6.4％至11.9％，并且快了1-3个数量级。 Comparing with alternative approaches for 3D detection 与3D检测的替代方法进行比较 在这一部分中，我们评估了一些基于CNN的基线方法以及使用2D蒙版的管道的烧蚀版本和变体。在表7的第一行中，我们显示了来自两个基于CNN的网络的3D框估计结果。基线方法在RGB-D图像的基本事实框上训练了VGG [32]模型，并采用与我们的主要方法相同的框参数和损失函数。第一行中的模型直接从vanilla RGB-D图像补丁中估计框位置和参数，而另一行（第二行）使用从COCO数据集中训练的FCN进行2D蒙版估计（如Mask- RCNN [ 14]），并且仅将遮罩区域中的特征用于预测。还可以通过减去2D蒙版中的中间深度来转换深度值。但是，与我们的主要方法相比，两个CNN基线得出的结果都差得多。 为了理解为什么CNN基线表现不佳，我们在图7中可视化了典型的2D蒙版预测。尽管估计的2D蒙版在RGB图像上以高质量显示，但2D蒙版中仍然有很多杂波和前景点。相比之下，我们的3D实例分割得到的结果要干净得多，这极大地简化了下一个模块的精细定位和边界框回归的过程。 在表7的第三行中，我们尝试使用没有3D实例细分模块的视锥体PointNet的烧蚀版本。毫不奇怪，该模型的结果比我们的主要方法差很多，这表明了我们的3D实例分割模块的关键作用。在第四行中，我们使用2D遮罩深度图（图7）中的点云代替3D分割进行3D框估计。但是，由于2D蒙版无法清晰地分割3D对象，因此其性能比3D分割（我们在第五行的主要方法）要差12％以上。另一方面，将2D和3D蒙版组合使用（在2D蒙版深度图上对点云进行3D分割）也显示出比我们的主要方法稍差的结果，这可能是由于不准确的2D蒙版预测导致的累积误差。 表7.比较2D和3D方法。2D遮罩来自RGB图像补丁上的FCN。3D遮罩来自于锥体点云上的PointNet。2D + 3D遮罩是PointNet在从2D遮罩深度图弹出的点云上生成的3D遮罩。 表8.点云归一化的影响。指标是IoU = 0.7的3D框估计精度。 表9. 3D框损失公式的影响。指标是IoU = 0.7的3D框估计精度。 Effects of point cloud normalization 点云归一化的影响 如图4所示，我们的锥体PointNet进行了一些关键的坐标转换以规范化点云，从而提高学习效率。表8显示了每个标准化步骤如何帮助3D检测。我们看到，视锥旋转（使视锥点具有更相似的XYZ分布）和蒙版质心减法（使对象点具有更小且规范的XYZ）都是至关重要的。此外，通过T-Net将目标点云与目标中心的额外对齐也大大提高了性能。 Effects of regression loss formulation and corner loss 回归损失公式和拐角损失的影响 在表9中，我们比较了不同的损失选项，并显示了“ cls-reg”损失（航向和尺寸回归的分类和残差回归方法）和正则化角损失的组合可获得最佳结果。 仅使用回归损失的原始基线（第一行）无法获得令人满意的结果，因为回归目标的范围较大（对象大小从0.2m到5m）。相比之下，其cls-reg损失和归一化版本（通过标题箱大小或模板形状大小归一化的残差）可获得更好的性能。在最后一行，我们显示正则化角损失进一步有助于优化。 Qualitative Results and Discussion 定性结果与讨论在图6中，我们可视化了锥体PointNet模型的代表性输出。我们看到，对于在合理距离内无遮挡的对象的简单情况（因此我们获得了足够的点数），我们的模型输出了非常准确的3D实例分割蒙版和3D边界框。其次，我们惊讶地发现我们的模型甚至可以从具有少量点的部分数据（例如平行停放的汽车）中正确预测出正确的无模式3D边框。 甚至人类也发现仅用点云数据注释此类结果非常困难。第三，在某些情况下，在具有大量靠近或甚至重叠2D边框的图像中看起来非常具有挑战性，当转换为3D空间时，定位变得容易得多（例如，第二行第三列中的P11）。 另一方面，我们确实观察到了几种故障模式，这些模式指示了未来工作的可能方向。第一个常见错误是由于稀疏点云（有时少于5个点）中的姿势和尺寸估计不准确。 我们认为图像特征可以极大地帮助实现。因为即使对于遥远的物体，我们也可以使用高分辨率的图像补丁。第二种挑战是在锥体中有多个来自同一类别的实例（例如两个人站在旁边）。由于我们当前的pipeline在每个视锥中都假定有单个感兴趣的对象，因此当出现多个实例时，可能会感到困惑，从而输出混合的分割结果。如果我们能够在每个视锥中提出多个3D边界框，则可以缓解该问题。第三，有时我们的2D检测器会由于昏暗的灯光或强烈的遮挡而错过物体。由于我们的视锥提案基于区域提案，因此如果没有2D检测，就不会检测到3D对象。但是，我们的3D实例分割和非模态3D框估计PointNet不仅限于RGB视图建议。如补充资料所示，相同的框架也可以扩展到鸟瞰中提出的3D区域。 Acknowledgement 致谢 作者要感谢Nuro Inc.的支持，ONR MURI赠款N00014-13-1-0341，NSF赠款DMS-1546206和IIS-1528025（三星GRO奖）以及Adobe，Amazon和Apple的礼物。 References[1] Kitti 3d object detection benchmark leader board. http://www.cvlibs.net/datasets/kitti/ eval_object.php?obj_benchmark=3d. Accessed: 2017-11-14 12PM. 2 [2] Kitti bird’s eye view object detection benchmark leader board. http://www.cvlibs.net/datasets/ kitti/eval_object.php?obj_benchmark=bev. Accessed: 2017-11-14 12PM. 2 [3] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.14 [4] X.Chen,K.Kundu,Z.Zhang,H.Ma,S.Fidler,andR.Urta- sun. Monocular 3d object detection for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2147–2156, 2016. 2, 6, 11 [5] X.Chen,K.Kundu,Y.Zhu,A.G.Berneshawi,H.Ma,S.Fi- dler, and R. Urtasun. 3d object proposals for accurate object class detection. In Advances in Neural Information Process- ing Systems, pages 424–432, 2015. 6 [6] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for autonomous driving. In IEEE CVPR, 2017. 2, 5, 6, 11, 12, 13 [7] Z. Deng and L. J. Latecki. Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth im- ages. In Conference on Computer Vision and Pattern Recog- nition (CVPR), volume 2, 2017. 2 [8] M.Engelcke,D.Rao,D.Z.Wang,C.H.Tong,andI.Posner. Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks. In Robotics and Au- tomation (ICRA), 2017 IEEE International Conference on, pages 1355–1361. IEEE, 2017. 1, 2 [9] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. Dssd: Deconvolutional single shot detector. arXiv preprint arXiv:1701.06659, 2017. 12 [10] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231–1237, 2013. 5 [11] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au- tonomous driving? the kitti vision benchmark suite. InConference on Computer Vision and Pattern Recognition (CVPR), 2012. 5 [12] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter- national conference on computer vision, pages 1440–1448, 2015. 12 [13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580–587. IEEE, 2014. 1 [14] K. He, G. Gkioxari, P. Dolla ́r, and R. Girshick. Mask r-cnn.arXiv preprint arXiv:1703.06870, 2017. 1, 3, 7 [15] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In NIPS 2015. 4 [16] J. Lahoud and B. Ghanem. 2d-driven 3d object detection in rgb-d images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4622– 4630, 2017. 2, 7 [17] B. Li. 3d fully convolutional network for vehicle detection in point cloud. arXiv preprint arXiv:1611.08069, 2016. 2, 5,6 [18] B. Li, T. Zhang, and T. Xia. Vehicle detection from 3d lidar using fully convolutional network. arXiv preprint arXiv:1608.07916, 2016. 2, 13 [19] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. Fpnn: Field probing neural networks for 3d data. arXiv preprint arXiv:1605.06240, 2016. 2 [20] T.-Y. Lin, P. Dolla ́r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection.arXiv preprint arXiv:1612.03144, 2016. 3, 12 [21] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dolla ́r. Focal loss for dense object detection. arXiv preprint arXiv:1708.02002, 2017. 12 [22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21–37. Springer, 2016. 12 [23] D. Maturana and S. Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems, September 2015. 1, 2 [24] A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka. 3d bounding box estimation using deep learning and geometry.arXiv preprint arXiv:1612.00496, 2016. 2, 5 [25] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation.Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 1, 2, 4, 5, 10, 11, 13 [26] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2016. 1, 2 [27] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space.arXiv preprint arXiv:1706.02413, 2017. 1, 2, 4, 5, 7, 10, 11,13, 14 [28] J. Ren, X. Chen, J. Liu, W. Sun, J. Pang, Q. Yan, Y.-W. Tai, and L. Xu. Accurate single stage detector using recurrent rolling convolution. In CVPR, 2017. 13 [29] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. InAdvances in neural information processing systems, pages 91–99, 2015. 2, 5, 12 [30] Z. Ren and E. B. Sudderth. Three-dimensional object detec- tion and layout prediction using clouds of oriented gradients. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1525–1533, 2016. 2, 7, 12 [31] G. Riegler, A. O. Ulusoys, and A. Geiger. Octnet: Learning deep 3d representations at high resolutions. arXiv preprint arXiv:1611.05009, 2016. 2 [32] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 7, 12, 13 [33] S. Song, S. P. Lichtenberg, and J. Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 567–576, 2015. 1, 5, 7, 12 [34] S. Song and J. Xiao. Sliding shapes for 3d object detection in depth images. In Computer Vision–ECCV 2014, pages 634–651. Springer, 2014. 2 [35] S. Song and J. Xiao. Deep sliding shapes for amodal 3d ob- ject detection in rgb-d images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 808–816, 2016. 2, 7 [36] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proc. ICCV, 2015. 1 [37] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. arXiv preprint arXiv:1707.02968, 1, 2017. 14 [38] D. Z. Wang and I. Posner. Voting for voting in online point cloud object detection. Proceedings of the Robotics: Science and Systems, Rome, Italy, 1317, 2015. 2 [39] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong. O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM Transactions on Graphics (TOG), 36(4):72, 2017. 2 [40] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015. 1,2 [41] Y. Xiang, W. Choi, Y. Lin, and S. Savarese. Data-driven 3d voxel patterns for object category recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1903–1911, 2015. 2 [42] S.-L. Yu, T. Westfechtel, R. Hamada, K. Ohno, and S. Ta- dokoro. Vehicle detection and localization on birds eye view elevation images using convolutional neural network. 2017 IEEE International Symposium on Safety, Security and Res- cue Robotics (SSRR), 2017. 5, 6]]></content>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习]]></title>
    <url>%2F2019%2F10%2F10%2FTensorflow%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[《Tensorflow：实战Google深度学习框架》 Tensorflow学习为了避免过拟合问题，非常常用的方法是：正则化 正则化的思想就是在损失函数中加入刻画模型复杂程度的指标。 假如用于刻画模型在训练数据上表现的损失函数为J(θ),那么在优化时不是直接优化J(θ)，而是优化J(θ)+λR(w)。其中R(w)刻画的是模型的复杂程度，而λ表示模型复杂损失在总损失中的比例。θ表示的是一个神经网络中的所有的参数，它包括边上的权重w和偏置项b。一般来说模型的复杂程度只由权重w决定。常用的刻画模型复杂程度的函数R(w)有两种 一种是L1正则化，计算公式是:$R(w)=||{w}||=\sum\limits_i$ R(w)=||{w_i}||=\sum\limits_i公式呢？2]]></content>
  </entry>
  <entry>
    <title><![CDATA[点云数据格式]]></title>
    <url>%2F2019%2F09%2F21%2F%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[PCD+PLY PCD格式文件头格式现有的文件结构因本身组成的原因不支持由PCL库引进n维点类型机制处理过程中的某些扩展，而PCD文件格式能够很好地补足这一点。PCD不是第一个支持3D点云数据的文件类型，尤其是计算机图形学和计算几何学领域，已经创建了很多格式来描述任意多边形和激光扫描仪获取的点云。包括下面几种格式： PLY是一种多边形文件格式，由Stanford大学的Turk等人设计开发； STL是3D Systems公司创建的模型文件格式，主要应用于CAD、CAM领域； OBJ是从几何学上定义的文件格式，首先由Wavefront Technologies开发； X3D是符合ISO标准的基于XML的文件格式，表示3D计算机图形数据； 其他许多种格式。 以上所有的文件格式都有缺点，在下一节会讲到。这是很自然的，因为它们是在不同时间为了不同的使用目的所创建的，那时今天的新的传感器技术和算法都还没有发明出来。 每一个PCD文件包含一个文件头，它确定和声明文件中存储的点云数据的某种特性。PCD文件头必须用ASCII码来编码。PCD文件中指定的每一个文件头字段以及ascii点数据都用一个新行（\n）分开了，从0.7版本开始，PCD文件头包含下面的字段： VERSION –指定PCD文件版本 FIELDS –指定一个点可以有的每一个维度和字段的名字。例如： 123456789FIELDS x y z # XYZ dataFIELDS x y z rgb # XYZ + colorsFIELDS x y z normal_xnormal_y normal_z # XYZ + surface normalsFIELDS j1 j2 j3 # moment invariants... SIZE –用字节数指定每一个维度的大小。例如： 1234567unsigned char/char has 1 byteunsigned short/short has 2 bytesunsignedint/int/float has 4 bytesdouble has 8 bytes TYPE –用一个字符指定每一个维度的类型。现在被接受的类型有 12345I – 表示有符号类型int8（char）、int16（short）和int32（int）；U – 表示无符号类型uint8（unsigned char）、uint16（unsigned short）和uint32（unsigned int）；F – 表示浮点类型。 COUNT –指定每一个维度包含的元素数目。例如，x这个数据通常有一个元素，但是像VFH这样的特征描述子就有308个。实际上这是在给每一点引入n维直方图描述符的方法，把它们当做单个的连续存储块。默认情况下，如果没有COUNT，所有维度的数目被设置成1。 WIDTH –用点的数量表示点云数据集的宽度。根据是有序点云还是无序点云，WIDTH有两层解释： 它能确定无序数据集的点云中点的个数（和下面的POINTS一样） 它能确定有序点云数据集的宽度（一行中点的数目） 注意：有序点云数据集，意味着点云是类似于图像（或者矩阵）的结构，数据分为行和列。这种点云的实例包括立体摄像机和时间飞行摄像机生成的数据。有序数据集的优势在于，预先了解相邻点（和像素点类似）的关系，邻域操作更加高效，这样就加速了计算并降低了PCL中某些算法的成本。 例如： WIDTH 640 # 每行有640个点 HEIGHT –用点的数目表示点云数据集的高度。类似于WIDTH ，HEIGHT也有两层解释： 它表示有序点云数据集的高度（行的总数） 对于无序数据集它被设置成1（被用来检查一个数据集是有序还是无序） 有序点云例子： WIDTH 640 # 像图像一样的有序结构，有640行和480列， HEIGHT 480 # 这样该数据集中共有640*480=307200个点 无序点云例子： WIDTH 307200 HEIGHT 1 # 有307200个点的无序点云数据集 VIEWPOINT–指定数据集中点云的获取视点。VIEWPOINT有可能在不同坐标系之间转换的时候应用，在辅助获取其他特征时也比较有用，例如曲面法线，在判断方向一致性时，需要知道视点的方位，视点信息被指定为平移（txtytz）+四元数（qwqxqyqz）。 默认值是：VIEWPOINT 0 0 0 1 0 0 0 POINTS–指定点云中点的总数。从0.7版本开始，该字段就有点多余了，因此有可能在将来的版本中将它移除 例子： POINTS 307200 #点云中点的总数为307200 DATA –指定存储点云数据的数据类型。从0.7版本开始，支持两种数据类型：ascii和二进制。 注意：文件头最后一行（DATA）的下一个字节就被看成是点云的数据部分了，它会被解释为点云数据。 警告：PCD文件的文件头部分必须以上面的顺序精确指定，也就是如下顺序： VERSION、FIELDS、SIZE、TYPE、COUNT、WIDTH、HEIGHT、VIEWPOINT、POINTS、DATA 之间用换行隔开。 数据存储类型在0.7版本中，.PCD文件格式用两种模式存储数据： 如果以ASCII形式，每一点占据一个新行： 1234567p_1p_2...p_n 注意：从PCL 1.0.1版本开始，用字符串“nan”表示NaN，此字符表示该点的值不存在或非法等。 如果以二进制形式，这里数据是数组（向量）pcl::PointCloud.points的一份完整拷贝，在Linux系统上，我们用mmap/munmap操作来尽可能快的读写数据，存储点云数据可以用简单的ascii形式，每点占据一行，用空格键或Tab键分开，没有其他任何字符。也可以用二进制存储格式，它既简单又快速，当然这依赖于用户应用。ascii格式允许用户打开点云文件，使用例如gunplot这样的标准软件工具更改点云文件数据，或者用sed、awk等工具来对它们进行操作。 相对其他文件格式的优势用PCD作为（另一种）文件格式可能被看成是没有必要的一项工作。但实际中，情况不是这样的，因为上面提到的文件格式无一能提高PCD文件的适用性和速度。PCD文件格式包括以下几个明显的优势： 存储和处理有序点云数据集的能力——这一点对于实时应用，例如增强现实、机器人学等领域十分重要； 二进制mmap/munmap数据类型是把数据下载和存储到磁盘上最快的方法； 存储不同的数据类型（支持所有的基本类型：char，short，int，float，double）——使得点云数据在存储和处理过程中适应性强并且高效，其中无效的点的通常存储为NAN类型； 特征描述子的n维直方图——对于3D识别和计算机视觉应用十分重要。 另一个优势是通过控制文件格式，我们能够使其最大程度上适应PCL，这样能获得PCL应用程序的最好性能，而不用把一种不同的文件格式改变成PCL的内部格式，这样的话通过转换函数会引起额外的延时。 注意：尽管PCD（点云数据）是PCL中的内部文件格式，pcl_io库也提供在前面提到的所有其他文件格式中保存和加载数据。 例子下面贴出了PCD文件的一个片段。把它留给读者以解析这些数据，看看它的组成，玩的愉快！ 12345678910111213# .PCD v.7 - Point Cloud Data file formatVERSION .7FIELDS x y z rgbSIZE 4 4 4 4TYPE F FFFCOUNT 1 1 1 1WIDTH 213HEIGHT 1VIEWPOINT 0 0 0 1 0 0 0POINTS 213DATA ascii0.93773 0.33763 0 4.2108e+060.90805 0.35641 0 4.2108e+06 PLY——多边形文件格式也被称为斯坦福三角格式 足够简单 典型的PLY对象定义仅仅是顶点的（x，y，z）三元组列表和由顶点列表中的索引描述的面的列表 文件结构这是一个典型的PLY文件的结构： 1234Header Vertex List Face List (lists of other elements) 标题是一系列回车终止的文本行，描述文件的其余部分。 标题包括每个元素类型的描述，包括元素的名称（例如“边”），对象中有多少这样的元素以及与该元素相关联的各种属性的列表。 在标题之后是每个元素类型的元素列表，按照标题中描述的顺序呈现 12345678910111213141516171819202122232425plyformat ascii 1.0 &#123; ascii/binary, format version number &#125;comment made by Greg Turk &#123; comments keyword specified, like all lines &#125;comment this file is a cubeelement vertex 8 &#123; define &quot;vertex&quot; element, 8 of them in file &#125;property float x &#123; vertex contains float &quot;x&quot; coordinate &#125;property float y &#123; y coordinate is also a vertex property &#125;property float z &#123; z coordinate, too &#125;element face 6 &#123; there are 6 &quot;face&quot; elements in the file &#125;property list uchar int vertex_index &#123; &quot;vertex_indices&quot; is a list of ints &#125;end_header &#123; delimits the end of the header &#125;0 0 0 &#123; start of vertex list &#125;0 0 10 1 10 1 01 0 01 0 11 1 11 1 04 0 1 2 3 &#123; start of face list &#125;4 7 6 5 44 0 4 5 14 1 5 6 24 2 6 7 34 3 7 4 0 此示例演示了标题的基本组件。 标题的每个部分都是以关键字开头的回车终止的ASCII字符串。 即使标头（“ply”和“end_header”）的开头和结尾都是这种形式。 字符“ply”必须是文件的前四个字符，因为它们作为文件的魔术数字。 12345element property property property ... 在“元素”行之后列出的属性定义属性的数据类型以及属性为每个元素显示的顺序。 属性可能有两种类型的数据类型：标量和列表。 以下是属性可能具有的标量数据类型的列表： 12345678910name type number of bytes---------------------------------------char character 1uchar unsigned character 1short short integer 2ushort unsigned short integer 2int integer 4uint unsigned integer 4float single-precision float 4double double-precision float 8 这些字节计数是重要的，并且不能在实现之间变化，以便这些文件可移植。 有一种使用列表数据类型的特殊形式的属性定义： property list 一个例子是上面的立方体文件： property list uchar int vertex_index 这意味着属性“vertex_index”首先包含一个无符号字符，表示该属性包含多少个索引，后跟一个包含多个整数的列表。 此变长列表中的每个整数都是顶点的索引。 另一个例子这是另一个立方体定义： 12345678910111213141516171819202122232425262728293031323334353637383940plyformat ascii 1.0comment author: Greg Turkcomment object: another cubeelement vertex 8property float xproperty float yproperty float zproperty uchar red &#123; start of vertex color &#125;property uchar greenproperty uchar blueelement face 7property list uchar int vertex_index &#123; number of vertices for each face &#125;element edge 5 &#123; five edges in object &#125;property int vertex1 &#123; index to first vertex of edge &#125;property int vertex2 &#123; index to second vertex &#125;property uchar red &#123; start of edge color &#125;property uchar greenproperty uchar blueend_header0 0 0 255 0 0 &#123; start of vertex list &#125;0 0 1 255 0 00 1 1 255 0 00 1 0 255 0 01 0 0 0 0 2551 0 1 0 0 2551 1 1 0 0 2551 1 0 0 0 2553 0 1 2 &#123; start of face list, begin with a triangle &#125;3 0 2 3 &#123; another triangle &#125;4 7 6 5 4 &#123; now some quadrilaterals &#125;4 0 4 5 14 1 5 6 24 2 6 7 34 3 7 4 00 1 255 255 255 &#123; start of edge list, begin with white edge &#125;1 2 255 255 2552 3 255 255 2553 0 255 255 2552 0 0 0 0 &#123; end with a single black line &#125; 该文件为每个顶点指定一个红色，绿色和蓝色的值。 为了说明vertex_index的可变长度属性，对象的前两个面是三角形，而不是单个正方形。 这意味着对象中的面数是7.这个对象还包含一个边列表。 每个边包含两个指向边缘的顶点的指针。 每个边缘也有一个颜色。 指定了上面定义的五个边，以突出显示文件中的两个三角形。 前四个边是白色的，它们围绕着两个三角形。 最后的边缘是黑色的，它是分隔三角形的边。 用户定义的元素上面的例子显示了使用三个元素：顶点，面和边。 PLY格式允许用户自己定义元素。 用于定义新元素的格式与顶点，面和边缘完全相同。 以下是定义材质属性的标题部分： 1234567891011121314element material 6property ambient_red uchar &#123; ambient color &#125;property ambient_green ucharproperty ambient_blue ucharproperty ambient_coeff floatproperty diffuse_red uchar &#123; diffuse color &#125;property diffuse_green ucharproperty diffuse_blue ucharproperty diffuse_coeff floatproperty specular_red uchar &#123; specular color &#125;property specular_green ucharproperty specular_blue ucharproperty specular_coeff floatproperty specular_power float &#123; Phong power &#125; 这些线将直接在顶点，面和边的规范之后出现在标题中。 如果我们希望每个顶点都有一个材质规范，我们可以将这一行添加到顶点属性的末尾： 1property material_index int 该整数现在是包含在文件中的材料列表的索引。一个新的应用程序的作者可能会发现几个要存储在PLY文件中的新元素。这种做法应该保持在最低限度。更好的是尝试将常用元素（顶点，面，边缘，材质）修改为新的用途，以便了解这些元素的其他程序可能有助于处理这些适应的元素。例如，将分子描述为球体和圆柱体的集合的应用程序。对于包含分子的PLY文件，将是诱人的定义球体和圆柱体元素。但是，如果我们为此使用顶点和边缘元素（将radius属性添加到每个元素），我们可以使用操纵和显示顶点和边的程序。显然，不应该为三角形和四边形创建特殊元素，而是使用面元素。如果程序不知道面和顶点之间的邻接（所谓的非共享顶点）怎么办？这就是每个三角形（说）纯粹是空间中三个位置的集合，没有概念是否有一些三角形有共同的顶点。这是一个相当普遍的情况。假设给定对象中有N个三角形，则应将3N顶点写入文件，然后再将N个面简单地连接到这些顶点。我们预计将编写一个在非共享和共享顶点文件之间进行转换的实用程序。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pointnet++理解]]></title>
    <url>%2F2019%2F09%2F17%2Fpointnet-%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/sinat_37011812/article/details/81945050 简介作者在先前的研究中提出Pointnet，此论文是Pointnet的改进版Pointnet++。提出改进的理由是因为Pointnet无法很好地捕捉由度量空间引起的局部结构问题，由此限制了网络对精细场景的识别以及对复杂场景的泛化能力。 Pointnet的基本思想是对输入点云中的每一个点，学习其对应的空间编码，之后再利用所有点的特征得到一个全局的点云特征，这里欠缺了对局部特征的提取及处理，比如说点云空间中临近点一般都具有相近的特征，同属于一个物体空间中的点的概率很大，就好比二维图像中，同一个物体的像素值都相近一样。 再者现实场景中的点云往往是书迷不同的，而Pointnet是基于均匀采样的点云进行训练的，导致了其在实际场景点云中的准确率下降。 Pointnet++就上述提出了改进，解决了两个问题： 如何对点云进行局部划分 如何对点云进行局部特征提取 上述两个问题其实是相互关联的。 实现细节 层级点云特征学习点集的特征提取由三部分组成，分别为Sampling layer、Grouping layer、Pointnet layer。 Sampling layer采样层在输入点云中选择一系列点，由此定义出局部区域的中心。采样算法使用迭代最远点采样方法iterative farthest point sampling（FPS）。FPS：先随机选择一个点，然后再选择离这个点最远的点作为起点，再继续迭代，知道选出需要的个数为止。 相比随机采样，能更完整的通过区域中心点采样到全局点云 Grouping layer目的是要构建局部区域，进而提取特征。思想就是利用临近点，并且论文中使用的是neighborhood ball，而不是KNN，是因为可以保证有一个fixed region scale，主要的指标还是距离distance。 Pointnet layer在如何对点云进行局部特征提取的问题上，利用原有的Pointnet就可以很好的提取点云的特征，由此在Pointnet++中，原先的Pointnet网络就成为了Pointnet++网络中的子网络，层级迭代提取特征。 点云密度不均匀时的鲁棒特征学习这里作者解决空间中点云的密度不均匀对特征学习带来的挑战，提出了两种grouping的方法，即如何提取不同尺度的局部patterns并按照局部点的密度去组合它们。称为密度自适应层 Multi-scale Grouping简单而有效的方式，直接对不同密度的点云特征（通过Pointnet提取后的）进行组合 不同密度的点云时通过对输入点云进行不同概率的dropout得到的 问题是计算量比较大 Multi-resolution Grouping分两部分，一部分直接用Pointnet从raw points上提取特征，另一部分是对subregion使用set abstraction得到的特征的集合。第一部分想当于是一个比较全局的部分，第二部分相当于是一个比较局部的部分，这里用两部分可以很好的控制全局区域密度。当局部区域密度比较小时，说明全局特征没有全局特征可靠，因此可以增加全局特征的权重。反之也是如此。这就相当于权重可以在密度的变化之中可以被学习到。 Segmentation在网络中输入不断被降采样而在segmentation中label都是针对原始点的，相当于需要做一个upsampling的动作。作者使用插值的方法再和之前的set abstraction中的feature做一个concatenate，inverse distance weighted average based on k nearest neighbors 结论Pointnet++的结构在3D point clouds上取得了state of art的水平，解决了如何处理采样不均匀的问题，也考虑了空间中点与点之间的距离度量，通过层级结构利用局部区域信息学习特征，网络结构更有效更鲁棒。]]></content>
      <tags>
        <tag>Pointnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令]]></title>
    <url>%2F2019%2F09%2F14%2Flinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一些用到的Linux命令 清华镜像pip install -i https://pypi.tuna.tsinghua.edu.cn/simple top命令TOP是一个动态显示过程,即可以通过用户按键来不断刷新当前状态。如果在前台执行该命令,它将独占前台,直到用户终止该程序为止。比较准确的说,top命令提供了实时的对系统处理器的状态监视。它将显示系统中CPU最“敏感”的任务列表。该命令可以按CPU使用。内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。 nvidia-smi 命令nvidia-smi 查看gpu的使用情况：]]></content>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PointNet理解]]></title>
    <url>%2F2019%2F08%2F16%2FPointNet%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[初学pointnet，一些重点笔记 一、三维深度学习简介 多视角（multi-view）：通过多视角二维图片组合三维物体，此方法将传统CNN应用于多张二维视角的图片，特征被view pooling procedure聚合起来形成三维物体 体素（volumetric）：通过将物体表现为空间中的体素进行类似于二维的三维卷积（例如，卷积核大小为5*5*5），是规律化的并且易于类比二维的，但同时因为多了一个维度出来，时间和空间复杂度都非常高，目前已经不是主流的方法了 点云（point clouds）：直接将三维点云抛入网络进行训练，数据量小。主要任务有分类、分割以及大场景下的语义分割 非欧式（manifold graph）：在流形或图的结构上进行卷积，三维点云可以表现为mesh结构，可以通过点对之间临接关系表现为图的结构。流形表达比较抽象，用到拉普拉斯特征什么的 二、点云存在的问题1.无序性：点云本质上是一长串店（n*3矩阵，其中n是点数）。在几何上，点的顺序不影响它在空间中对整体形状的表示，例如，相同的点云可以由两个完全不同的矩阵表示。如下图左边所示： 我们希望得到的效果图如下图右边：N代表点云个数，D代表每个点的特征维度，不论点云顺序怎样，希望得到相同的特征提取结果 我们知道，网络的一般结构是：提取特征-特征映射-特征图压缩（降维）-全连接 下图中x代表点云中的某个点，h代表特征提取层，g叫做对称方法，r代表更高维特征提取，最后接一个sofmax分类。g可以是maxpooling或sumpooling，也就是说，最后的D维特征对每一维都选取N个点中对应的最大特征值或特征值总和，这样就可以通过g来解决无序性问题。pointnet采用了max-pooling策略 2.旋转性：相同的点云在空间中经过了一定的刚性变化（旋转或平移），坐标发生变化，如下图所示： 我们希望不论点云在怎样的坐标系下呈现，网络都能正确的识别出。这个问题可以通过STN(spacial transform networks)来解决。二维的变换方法可以参考这里，三维不太一样的是点云是个不规则的结构（无序，无网格），不需要重采样过程。pointnet通过学习一个矩阵来达到对目标最有效的变换。 三、pointnet网络结构详解先来看看网络的两个亮点： 空间变换网络（STN）解决旋转问题：三维的STN可以通过学习点云本身的位姿信息学习到一个最有利于网络进行分类或分割的D*D旋转矩阵（D代表特征维度，pointnet中D采用3和64）。至于其中的原理，我的理解是，通过控制最后的loss来对变换矩阵进行调整，pointnet并不关心最后真正做了什么变换，只要有利于最后的结果都可以。pointnet采用了两次STN，第一次input transform是对空间中的点云进行调整，直观上理解是旋转出一个更有利于分类或分割的角度，比如，把物体转到正面；第二次feature transform是对提取出的64维特征进行对齐，即在特征层面对点云进行变换 maxpooling解决无序性问题：网络对每个点进行了一定程度的特征提取之后，maxpooling可以对点云的整体提取出global feature 再来看网络结构： 其中mlp是通过共享权重的卷积实现，第一层卷积核大小是13（因为每个点的维度是xyz），之后的每一层卷积核大小都是1\1。即特征提取层知识把每个点连接起来而已。经过两个STN和两个mlp之后，对每个点提取1024维特征，经过maxpooling变成1*1024的全局特征。再经过一个mlp(代码中运用全连接)得到k个score。分类网络最后接的loss是softmax 四、pointnet代码详解重点已经框出 网络模型部分 变换矩阵部分，以第一个STN为例]]></content>
      <tags>
        <tag>PointNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度图、网格、体素、点云]]></title>
    <url>%2F2019%2F08%2F16%2F%E6%B7%B1%E5%BA%A6%E5%9B%BE%E3%80%81%E7%BD%91%E6%A0%BC%E3%80%81%E4%BD%93%E7%B4%A0%E3%80%81%E7%82%B9%E4%BA%91%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/lainey/p/8547056.html 深度图Depth map 深度图是一张2D图片，每个像素都记录了从视点到遮挡物表面（遮挡物就是阴影生成物体）的距离，这些像素对应的顶点对于观察者来说是“可见的”。 Depth map中像素点记录的深度值为lenth1；然后从视点出现，计算物体顶点v到视点的距离，记为lenth2；比较二者大小，来确定“v”是否被遮挡。该术语的同义词有depth buffer，Z-buffer，Z-buffering和Z-depth。这里的“Z”是相对于相机（即视点）视图中心轴而言的，也就是相机的z轴线，而不是场景的绝对坐标中的z轴线。 用途 模拟在一个场景中的密度均匀的半透明介质效果-如雾，烟或大量的水 模拟场景表面的深度域（depth of field (DOF)） 可用于高效的变形体碰撞检测 体素（立体像素）体素或立体像素（voxel），是体积像素（volume pixel）的简称。概念上类似二维空间的最小单位——像素，像素用在二维电脑图像的视频数据上。体积像素一如其名，是数字数据于三维空间分区上的最小单位，应用于三维成像、科学数据与医学视频等领域。有些真正的三维显示器运用体素来描述他们的分辨率，举例说：可以显示512×512×512体素的显示器。 如同像素，体素本身并不含有空间中位置的数据（即他们的坐标），然而却可以从他们相对于其他体素的位置推敲，意即他们在构成单一张体积视频的数据结构中的位置。 网格多边形网络（Polygon mesh）是三维计算机图形学中表示多面体形状的顶点与多边形的集合,它也叫作非结构网格。 这些网格通常由三角形、四边形或者其他的简单凸多边形组成，这样可以简化渲染过程。但是，网格也可以包括带有空洞的普通多边形组成的物体。 非结构网格内部表示的例子有： 一组顶点的简单列表，它们带有表示那些顶点组成多边形的信息列表；另外可能带有表示空洞的附加信息 顶点列表 + 边界列表（一对索引信息）+ 连接边界的多边形列表 翼边数据结构 根据应用程序的不同所选择的数据结构也有所不同：三角形的处理要比普通多边形的处理更加简单，尤其是在计算几何中更是这样。对于优化的算法，可能需要快速访问边线或者相邻表面这样的拓扑信息，这样就需要如翼边表示这样更加复杂的结构。 点云（维基百科）点云（point cloud）是指通过3D扫描器所取得至资料形式。 扫描资料以点的型式记录，每一个点包含有三维座标，有些可能含有色彩资讯（R,G,B）或物体反射面强度。 点云数据除了具有几何位置以外，还有强度（Intensity）信息，强度信息的获取是激光扫描仪接受装置采集到的回波强度，此强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关。点云也是逆向工程中通过仪器测量外表的点数据集合。 在电脑动画领域，皮克斯的玩具总动员3使用了点云技术。 点云应用深度学习面临的挑战： 非结构化数据，不变性排列，点云数据量上的变化(不同传感器上点云的数量变化很大) 点云数据方面的挑战： 缺少数据：扫描的模型通常被遮挡，部分数据丢失 噪音：所有传感器都是嘈杂的。有几种类型的噪声，包括点云扰动和异常值。这意味着一个点有一定的概率位于它被采样的地方(扰动)附近的某一半径范围内，或者它可能出现在空间的任意位置(异常值) 旋转：一辆车向左转，同一辆车向右转，会有不同的点云代表同一辆车 在点云上直接用深度学习的方法是将数据转换成体积表示，比如体素网格，然后就可以用3D滤波器来训练CNN，但是体积数据会变得非常大，3D CNN处理会非常慢，所以需要妥协到较低的分辨率，就会带来量化误差的代价。 针对无序点云数据的深度学习方法研究进展缓慢，主要有三个方面： 点云具有无序性 点云具有稀疏性 —— 在KITTI数据集中，如果把原始的激光雷达点云投影到对应的彩色图像上，大概只有3%的像素才有对应的雷达点。这种极强的稀疏性让基于点云的高层语义感知变得尤其困难。 点云信息量有限 —— 点云的数据结构就是一些三维空间的点坐标构成的点集，本质是对三维世界几何形状的低分辨率重采样，因此只能提供片面的几何信息 举例说明 iphoneX 3D结构光双摄 用的是PrimeSense的结构光深度重建方案(Depth),和普通的彩色RGB不同，深度摄像头输出的时RGBD图像，多了一个深度通道，深度图像看起来是这样的：由深度图可以得到点云，进而得到网格（mesh）效果如下： 3D 重建后的人脸，比 2D人脸多了很多信息，识别显然会更准确 由于结构光的特性，每一帧数据都能重建出完整的 3D 模型，速度也非常快，才能适应像手机解锁这样的应用 结构光方案因为自带光源，所以天黑的时候也能用 最大池化 max pooling https://blog.csdn.net/u012193416/article/details/79432668 池化操作时在卷积神经网络中经常采用过的一个基本操作，一般在卷积层后面都会接一个池化操作，但是近些年比较主流的ImageNet上的分类算法模型都是使用的max-pooling，很少使用average-pooling，这对我们平时设计模型时确实有比较重要的参考作用，但是原因在哪里呢？ 通常来讲，max-pooling的效果更好，虽然max-pooling和average-pooling都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性，根据相关理论，特征提取的误差主要来自两个方面： 邻域大小受限造成的估计值方差增大 卷积层参数误差造成估计均值的偏移 一般来说，average-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。average-pooling更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说DenseNet中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。 但是average-pooling在全局平均池化操作中应用也比较广，在ResNet和Inception结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一位向量。 max-pooling和average-pooling的使用性能对于我们设计卷积网络还是很有用的，虽然池化操作对于整体精度提升效果也不大，但是在减参，控制过拟合以及提高模型性能，节约计算力上的作用还是很明显的，所以池化操作时卷积设计上不可缺少的一个操作。]]></content>
      <tags>
        <tag>点云</tag>
      </tags>
  </entry>
</search>
